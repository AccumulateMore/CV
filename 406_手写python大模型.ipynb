{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手写python大模型.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据 2025-06-30 13:18:55.233099\n",
      "内容长度 2317\n",
      "分词 2025-06-30 13:18:55.235095\n",
      "训练 2025-06-30 13:18:55.268005\n",
      "Epoch: 1/10, Batch: 0, Avg Loss:6.4988\n",
      "Epoch1 finished, Average Loss:4.7539\n",
      "Epoch: 2/10, Batch: 0, Avg Loss:3.0506\n",
      "Epoch2 finished, Average Loss:1.8768\n",
      "Epoch: 3/10, Batch: 0, Avg Loss:0.8697\n",
      "Epoch3 finished, Average Loss:0.5106\n",
      "Epoch: 4/10, Batch: 0, Avg Loss:0.3464\n",
      "Epoch4 finished, Average Loss:0.2590\n",
      "Epoch: 5/10, Batch: 0, Avg Loss:0.1770\n",
      "Epoch5 finished, Average Loss:0.2028\n",
      "Epoch: 6/10, Batch: 0, Avg Loss:0.1753\n",
      "Epoch6 finished, Average Loss:0.1774\n",
      "Epoch: 7/10, Batch: 0, Avg Loss:0.1516\n",
      "Epoch7 finished, Average Loss:0.1606\n",
      "Epoch: 8/10, Batch: 0, Avg Loss:0.1537\n",
      "Epoch8 finished, Average Loss:0.1558\n",
      "Epoch: 9/10, Batch: 0, Avg Loss:0.1285\n",
      "Epoch9 finished, Average Loss:0.1462\n",
      "Epoch: 10/10, Batch: 0, Avg Loss:0.1453\n",
      "Epoch10 finished, Average Loss:0.1400\n",
      "测试 2025-06-30 13:22:54.040832\n",
      "\n",
      "Generated Text:\n",
      "孙悟空在西方雨师山上挑战蝎子精，封印住西游三宝，只有获得蝎子精的“观音灵签”四个跟着说:“观音灵签”和把和尚去，垂才到此天山妖怪天通才到此宵。”才到贵处“，让唐僧上雷公僧说:“观音灵饮坐去，岂不得妖怪脑的神\n"
     ]
    }
   ],
   "source": [
    "# 导入处理日期时间的模块\n",
    "import datetime\n",
    "# 导入操作系统接口模块，用于环境变量、路径等操作\n",
    "import os\n",
    "# 从 pathlib 库中导入 Path 类，用于进行跨平台的文件路径处理\n",
    "from pathlib import Path\n",
    "\n",
    "# 导入 PyTorch 库及其子模块\n",
    "# PyTorch 主模块\n",
    "import torch             \n",
    "# 包含神经网络模块\n",
    "import torch.nn as nn        \n",
    "# 提供优化器功能\n",
    "import torch.optim as optim          \n",
    "# 导入 NumPy，用于科学计算（如数学函数）\n",
    "import numpy as np                          \n",
    "\n",
    "# 定义字符级别的分词器类（Tokenizer）\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text):\n",
    "        # 创建排序后的去重字符列表作为词汇表\n",
    "        self.vocab = sorted(list(set(text)))       \n",
    "        # 计算词汇表大小（字符数量）\n",
    "        self.vocab_size = len(self.vocab)                  \n",
    "        # 字符到索引的映射字典\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}  \n",
    "        # 索引到字符的映射字典\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(self.vocab)}  \n",
    "\n",
    "    def encode(self, text):\n",
    "        # 将输入文本的每个字符转换为索引\n",
    "        return [self.char_to_idx[char] for char in text]             \n",
    "\n",
    "    def decode(self, indices):\n",
    "        # 将索引列表还原为字符串文本\n",
    "        return ''.join([self.idx_to_char[idx] for idx in indices])   \n",
    "\n",
    "# 定义位置编码模块，用于为序列中的每个位置添加唯一信息\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        # 调用父类构造函数\n",
    "        super().__init__()              \n",
    "        # 创建一个形状为 (max_len, d_model) 的全零张量\n",
    "        pe = torch.zeros(max_len, d_model)                           \n",
    "        # 创建位置索引 (max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  \n",
    "        # 计算分母频率项\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))  \n",
    "\n",
    "        # 对偶数维度使用 sin 编码\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)               \n",
    "         # 对奇数维度使用 cos 编码\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)                \n",
    "        # 增加 batch 维度，形状变为 (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)                      \n",
    "        # 注册为 buffer，模型保存时包含但不更新\n",
    "        self.register_buffer('pe', pe)                               \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 截取对应长度位置编码，加到输入上\n",
    "        return x + self.pe[:, :x.size(1)]                            \n",
    "\n",
    "# 简化版 Transformer Decoder 层（本例未使用）\n",
    "class SimpleDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dim_feedforward, dropout):\n",
    "        # 初始化父类\n",
    "        super().__init__()   \n",
    "        # 自注意力层\n",
    "        self.self_atten = nn.MultiheadAttention(d_model, n_head, dropout=dropout, batch_first=True)  \n",
    "         # 第一个 LayerNorm\n",
    "        self.norm1 = nn.LayerNorm(d_model)   \n",
    "        # 前馈神经网络\n",
    "        self.ffn = nn.Sequential(                                    \n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        # 第二个 LayerNorm\n",
    "        self.norm2 = nn.LayerNorm(d_model)    \n",
    "        # Dropout 防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)                           \n",
    "\n",
    "    def forward(self, tgt, tgt_mask=None):\n",
    "        # 执行自注意力\n",
    "        attn_output, _ = self.self_atten(tgt, tgt, tgt, attn_mask=tgt_mask)  \n",
    "        # 残差连接后加入 Dropout\n",
    "        tgt = tgt + self.dropout(attn_output)           \n",
    "        # 第一次归一化\n",
    "        tgt = self.norm1(tgt)     \n",
    "        # 前馈输出\n",
    "        ffn_output = self.ffn(tgt)            \n",
    "         # 再次残差连接\n",
    "        tgt = tgt + self.dropout(tgt)    \n",
    "        # 第二次归一化\n",
    "        tgt = self.norm2(tgt)                                       \n",
    "        return tgt\n",
    "\n",
    "# 定义主模型：字符级语言模型\n",
    "class SimleTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_head, num_layers, dim_feedforward, dropout, max_len):\n",
    "        # 初始化父类\n",
    "        super().__init__()               \n",
    "        # 字符索引嵌入为 d_model 维向量\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)      \n",
    "        # 添加位置编码模块\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)     \n",
    "        # 单层解码器\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, n_head, dim_feedforward, dropout, batch_first=True)  \n",
    "        # 堆叠多个解码层\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers) \n",
    "        # 最终线性层输出词表概率\n",
    "        self.fc = nn.Linear(d_model, vocab_size)                    \n",
    "\n",
    "        # 保存模型维度\n",
    "        self.d_model = d_model       \n",
    "        # 初始化模型参数\n",
    "        self.init_weights()                                         \n",
    "\n",
    "    def init_weights(self):\n",
    "        # 设置初始化范围\n",
    "        initrange = 0.1                  \n",
    "        # 初始化嵌入层权重\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange) \n",
    "        # 将线性层偏置设为 0\n",
    "        self.fc.bias.data.zero_()    \n",
    "        # 初始化线性层权重\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)         \n",
    "\n",
    "    def forward(self, src, tgt_mask=None):\n",
    "        # 将输入索引嵌入并缩放\n",
    "        src_emb = self.embedding(src) * np.sqrt(self.d_model)   \n",
    "        # 加入位置编码\n",
    "        src_emb = self.pos_encoder(src_emb)             \n",
    "        # 使用零向量作为 Encoder 输出（Decoder-only）\n",
    "        memory = torch.zeros_like(src_emb)                   \n",
    "        # 解码器前向传播\n",
    "        output = self.transformer_decoder(tgt=src_emb, memory=memory, tgt_mask=tgt_mask)  \n",
    "        # 映射为词表大小输出\n",
    "        output = self.fc(output)                                    \n",
    "        return output\n",
    "\n",
    "    def generate_square_mask(self, size):\n",
    "        # 上三角掩码防止看到未来\n",
    "        mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)  \n",
    "        return mask\n",
    "\n",
    "# 文本数据集定义：将文本转换为序列对用于训练\n",
    "class TextDataset:\n",
    "    def __init__(self, text, tokenizer, seq_len):\n",
    "        # 保存分词器对象\n",
    "        self.tokenizer = tokenizer      \n",
    "        # 将文本转为字符索引列表\n",
    "        self.indexed_text = tokenizer.encode(text)     \n",
    "        # 设置序列长度\n",
    "        self.seq_len = seq_len                                      \n",
    "\n",
    "    def __len__(self):\n",
    "        # 可构造的训练样本数量\n",
    "        return len(self.indexed_text) - self.seq_len                \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 输入序列\n",
    "        input_indices = self.indexed_text[idx:idx + self.seq_len]   \n",
    "        # 目标序列为输入向后移动一位\n",
    "        target_indices = self.indexed_text[idx + 1:idx + self.seq_len + 1]  \n",
    "        # 转换为张量\n",
    "        return torch.tensor(input_indices), torch.tensor(target_indices)    \n",
    "\n",
    "# 定义训练模型的主循环函数\n",
    "def train_model(model, dataset, tokenizer, epochs, batch_size, seq_len, learning_rate, device):\n",
    "    # 将模型移动到设备（CPU/GPU）\n",
    "    model.to(device)      \n",
    "    # 设置为训练模式\n",
    "    model.train()                     \n",
    "    # 使用 AdamW 优化器\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)   \n",
    "    # 使用交叉熵损失\n",
    "    criterion = nn.CrossEntropyLoss()                               \n",
    "    # 数据加载器\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, drop_last=True)  \n",
    "\n",
    "    # 遍历每个 epoch\n",
    "    for epoch in range(epochs):                                     \n",
    "        total_loss = 0\n",
    "        # 遍历每个 batch\n",
    "        for batch_idx, (input_seq, target_seq) in enumerate(dataloader):  \n",
    "            # 移动数据到设备\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)  \n",
    "            # 生成因果掩码\n",
    "            src_mask = model.generate_square_mask(seq_len).to(device)           \n",
    "\n",
    "            # 清除梯度\n",
    "            optimizer.zero_grad()          \n",
    "            # 模型前向传播\n",
    "            output = model(input_seq, tgt_mask=src_mask)       \n",
    "            # 计算损失\n",
    "            loss = criterion(output.view(-1, tokenizer.vocab_size), target_seq.view(-1))  \n",
    "\n",
    "            # 反向传播\n",
    "            loss.backward()     \n",
    "            # 优化更新\n",
    "            optimizer.step()                                        \n",
    "\n",
    "            # 累积损失\n",
    "            total_loss += loss.item()      \n",
    "            # 每 100 批输出一次平均损失\n",
    "            if batch_idx % 100 == 0:                                \n",
    "                avg_loss = total_loss / (batch_idx + 1)\n",
    "                print(f'Epoch: {epoch+1}/{epochs}, Batch: {batch_idx}, Avg Loss:{avg_loss:.4f}')\n",
    "\n",
    "        # 当前 epoch 平均损失\n",
    "        avg_epoch_loss = total_loss / len(dataloader)               \n",
    "        print(f'Epoch{epoch+1} finished, Average Loss:{avg_epoch_loss:.4f}')     \n",
    "\n",
    "# 定义文本生成函数（给定起始字符生成序列）\n",
    "def generate_text(model, tokenizer, start_text, max_len, device, temperature=1.0):\n",
    "    # 移动模型到设备\n",
    "    model.to(device)          \n",
    "    # 设置为评估模式\n",
    "    model.eval()                    \n",
    "    # 将起始文本编码为索引列表\n",
    "    input_indices = tokenizer.encode(start_text)                   \n",
    "    # 添加 batch 维度并转为张量\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)  \n",
    "\n",
    "    # 初始化生成结果列表\n",
    "    generated_indices = input_indices.copy()                        \n",
    "\n",
    "    # 不记录梯度，节省内存\n",
    "    with torch.no_grad():      \n",
    "        # 最长生成 max_len 个字符\n",
    "        for _ in range(max_len):                        \n",
    "            # 动态生成掩码\n",
    "            src_mask = model.generate_square_mask(input_tensor.size(1)).to(device)  \n",
    "            # 前向传播获取 logits\n",
    "            output = model(input_tensor, tgt_mask=src_mask)         \n",
    "\n",
    "            # 获取最后一个位置的输出向量\n",
    "            last_output = output[:, -1, :]                          \n",
    "            # 应用 softmax 获取概率分布\n",
    "            output_probs = nn.functional.softmax(last_output / temperature, dim=-1)  \n",
    "            # 从概率中采样下一个字符索引\n",
    "            next_token_idx = torch.multinomial(output_probs, num_samples=1).item()  \n",
    "\n",
    "            # 添加新字符\n",
    "            generated_indices.append(next_token_idx)           \n",
    "            # 更新输入张量\n",
    "            input_tensor = torch.tensor([generated_indices]).to(device)  \n",
    "\n",
    "    # 解码生成的索引为字符串\n",
    "    generate_text = tokenizer.decode(generated_indices)             \n",
    "    return generate_text\n",
    "\n",
    "# 主程序入口\n",
    "if __name__ == '__main__':\n",
    "    # 输出当前时间\n",
    "    print('加载数据', datetime.datetime.now())                      \n",
    "\n",
    "    # 定义文本目录\n",
    "    book_dir = Path(r'test')      \n",
    "    # 初始化文本内容变量\n",
    "    text_data = ''             \n",
    "    # 遍历所有 txt 文件\n",
    "    for book in book_dir.glob('*.txt'):                         \n",
    "        # 读取文本并追加到 text_data\n",
    "        text_data += book.read_text(encoding='utf-8')               \n",
    "\n",
    "    # 打印文本总长度\n",
    "    print('内容长度', len(text_data))       \n",
    "    # 输出时间戳\n",
    "    print('分词', datetime.datetime.now())       \n",
    "    # 初始化字符分词器\n",
    "    tokenizer = CharTokenizer(text_data)                            \n",
    "\n",
    "    # 设置模型超参数\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    d_model = 128\n",
    "    n_head = 8\n",
    "    num_layers = 8\n",
    "    dim_feedforward = 256\n",
    "    dropout = 0.1\n",
    "    seq_len = 32\n",
    "    max_len = 512\n",
    "\n",
    "    # 设置训练超参数\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.001\n",
    "    # 检查是否使用 GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "    # 初始化模型\n",
    "    model = SimleTransformerLM(vocab_size, d_model, n_head, num_layers, dim_feedforward, dropout, max_len)  \n",
    "    # 创建数据集对象\n",
    "    dataset = TextDataset(text_data, tokenizer, seq_len)         \n",
    "    # 输出开始训练时间\n",
    "    print('训练', datetime.datetime.now())                          \n",
    "    # 开始训练\n",
    "    train_model(model, dataset, tokenizer, epochs, batch_size, seq_len, learning_rate, device)  \n",
    "\n",
    "    # 输出测试开始时间\n",
    "    print('测试', datetime.datetime.now())          \n",
    "    # 设置起始文本\n",
    "    start_text = '孙悟空'                                           \n",
    "    # 文本生成\n",
    "    generate_text = generate_text(model, tokenizer, start_text, max_len=100, device=device, temperature=1.0)  \n",
    "    # 输出提示信息\n",
    "    print('\\nGenerated Text:')                                     \n",
    "    # 打印生成的文本\n",
    "    print(generate_text)                                           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "277.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
